--model_name_or_path meta-llama/Meta-Llama-3-8B
--cache_dir models/hf_models/
--task summarization
--task_samples_path assets/summarization/task_samples/32-mixed-100/
--output_dir model_ckpts/summarization/lora/llama-3-8b-lora-32-mixed-100-seed_9999/
--lora_r 64
--lora_alpha 64
--lora_dropout 0.1
--lora_bias none
--batch_size 1
--use_bf16
--weight_decay 0.05
--learning_rate 1e-4
--grad_acc_steps 16
--num_epochs 3
--logging_strategy steps
--logging_steps 1
--save_strategy epoch
--save_only_model
--optim adamw_torch
--warmup_ratio 0.1
--num_workers 8
--save_total_limit 1
--random_seed 9999
--hf_token_path hf-llama.privkey