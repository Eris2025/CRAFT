--model_name_or_path meta-llama/Meta-Llama-3-8B
--cache_dir models/hf_models/
--task bioqa
--task_samples_path assets/bioqa/task_samples/8-mixed-25k/
--output_dir model_ckpts/bioqa/lora/llama-3-8b-lora-8-mixed-25k-seed_9999/
--lora_r 64
--lora_alpha 64
--lora_dropout 0.1
--lora_bias none
--batch_size 16
--use_bf16
--weight_decay 0.05
--learning_rate 1e-4
--grad_acc_steps 1
--num_epochs 3
--logging_strategy steps
--logging_steps 1
--save_strategy epoch
--save_only_model
--optim adamw_torch
--warmup_ratio 0.1
--num_workers 8
--save_total_limit 1
--random_seed 9999
--hf_token_path hf-llama.privkey